{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report for Part 3\n",
    "\n",
    "###### *Author: Jason*\n",
    "---\n",
    "\n",
    "1. Do you have data fully in hand and if not, what blockers are you facing?\n",
    "\n",
    "Ans: \n",
    "\n",
    "\n",
    "    Yes I have. The data that I have scraped from Yelp with the search term, \"coffee\", and location, \"Singapore\" yielded 987 unique coffee-drinking branches, with only around 7,076 ratings and 6,292 reviews. \n",
    "    \n",
    "    \n",
    "    I have noticed that I am unable to scrape past 987 coffee-drinking outlets using the Yelp API token and I am also unable to browse past 900+ outlets on the Yelp website (error page returned on browser). In a bid to yield more information (possibly for Content-based Filtering later on) on each coffee-drinking outlet, such as tags like \"Good for Family\", \"Air-Conditioned\", \"Offers Takeout\", \"Parking Garage\", \"Caters Yes\", \"Good for Working\" etc., I have written an email to Yelp to request for permission to obtain such information using the API token but to no avail - they did not reply my email sent weeks ago. \n",
    "    \n",
    "    \n",
    "    As some online resources mentioned that at least 10,000 reviews and/or ratings is sufficient for building a recommender system, I am considering abandoning my scraped dataset in favor of already established datasets used rigorously for building recommenders (such as MovieLens, where subsets of it can have at least 100,000 ratings and/or reviews), to demonstrate my understanding of data science concepts pertaining to the building of recommenders for this capstone project for a start. \n",
    "    \n",
    "    \n",
    "    Although the MovieLens Dataset has already been used in class one or more times demonstrating very foundational and conceptual knowledge surrounding recommenders, not enough was presented for a student to be able to build a production-standard hybrid (collaborative and content-based filtering) recommender, which I envision I will be able to add value on by attempting to build one for this capstone project.\n",
    "\n",
    "\n",
    "2. Have you done a full EDA on all of your data?\n",
    "\n",
    "\n",
    "Ans:\n",
    "\n",
    "\n",
    "    Yes I have, on the limited amount of Yelp data scraped both from BeautifulSoup and Yelp API token. They are shown as images below.\n",
    "\n",
    "<img src=\"../../yelp_data/sg_coffee_places_reviews.png\"/>\n",
    "\n",
    "    The wordcloud image above shows some of the common words gleaned from the limited number of reviews scraped and they are in line with what would be expected with the appearance of relevant generic words like \"coffee\", \"cafe\", \"drinks\", and more specific ones like \"dessert\", \"sweet\", \"cream\", \"pasta\", \"bread\", \"egg\", \"breakfast\", \"lunch\" etc. for users looking to have a little something else to accompany their coffee etc.\n",
    "    \n",
    "\n",
    "    Examined the numerical summary of the ratings as shown below.\n",
    "\n",
    "<img src=\"../../yelp_data/yelp_numsum.png\"/>\n",
    "\n",
    "    Given that the mean of ratings is more than median, the ratings are skewed to the right, with a min of 1 and max of 5. It is evident that most of the ratings fall between 4 and 5 and the ratings are not evenly distributed across all 5 rating scores (1 to 5), which means predictions derived from this dataset will be skewed. This is further exemplified by the ```value_counts(normalize=True)``` distribution plot below.\n",
    "\n",
    "<img src=\"../../yelp_data/yelp_distsum.png\"/>\n",
    "\n",
    "\n",
    "3. Have you begun the modeling process? How accurate are your predictions so far?\n",
    "\n",
    "Ans:\n",
    "\n",
    "\n",
    "    No-Have only conducted memory-based item-item collaborative filtering on the Yelp dataset, yielding the following results, which, as expected, did not represent discernible recommendations (screenshot below, demonstrating with an arbitrary search term, \"sweet\" set. \"Cafe Del Mar Sentosa\", which appears to be the only recommendable option when it comes to sweet-themed coffee-drinking places, is actually permanently closed (according to a simple Google search) even though it is listed as open based on Yelp data scraped with the Yelp API token. This constitutes a drawback of building a recommendation system based on data scraped from an online source once - it is not updated along with the online Yelp database, and will hence yield obsolete and inaccurate recommendations if deployed. Despite researching online, I am still unsure how memory-based collaborative filtering can be evaluated...\n",
    "    \n",
    "    \n",
    "<img src=\"../../yelp_data/mem_based_item-item_initial1.png\"/>\n",
    "<img src=\"../../yelp_data/mem_based_item-item_initial2.png\"/>\n",
    "\n",
    "4. What blockers are you facing, including processing power, data acquisition, modeling difficulties, data cleaning, etc.? How can we help you overcome those challenges?\n",
    "\n",
    "Ans:\n",
    "\n",
    "\n",
    "        Although the cleaned data I have accumulated from Yelp is insufficient for basic recommenders, the initial amount of data scraped yielded a few million rows (containing NaNs due to pd.melt()) and this caused my laptop to lag a lot. As I will be using a more sound and already established dataset which has at least 100,000 ratings and/or reviews, processing power may potentially be a concern.. Yes, modeling difficulties outline the most major concern I have at this juncture. Over the past few weeks of researching online, I have learnt the following: A good Model-based Collaborative Filter utilizes the dot product of user- and item-latent factor dense vectors as inputs (somehow) into an algorithm such as OLS/ALS, and together with added regularization terms, seek to learn the parameters for the user- and item-latent factor vectors that minimize loss via gradient descent. \n",
    "        \n",
    "<img src=\"../../movielens_data/model-based_ALS_OLS.png\"/>\n",
    "        \n",
    "        \n",
    "        I am entirely unsure how I should organize a dataset to produce the required inputs for such a model to churn out the predicted ratings of a user - do I have to use SVD to decompose the user-item interaction matrix (containing user ratings for the items) into the initial user- and item-latent factor dense vectors which will then serve as inputs into the OLS/ALS algorithm?? Perhaps the GA lecturer can walk me through a basic use case for incorporating such a OLS/ALS model for model-based collaborative filtering so that I know at least the guiding principles behind implementing it. \n",
    "        \n",
    "        \n",
    "<img src=\"../../movielens_data/content-based_user-centered_linreg.png\"/>      \n",
    "        \n",
    "        \n",
    "        For content-based filtering (above, \"User-centred linear regression\" screenshot) although I got the rough idea that I could use the item feature vector (eg. features of movies gleaned through the movielens dataset such as genres etc.) as input (Yj) into the OLS/ALS algorithm that will eventually predict user ratings but as this was not taught in class, I would appreciate if the GA lecturer could walk me through a basic example of this too.\n",
    "\n",
    "\n",
    "5. Have you changed topics since your lightning talk? Since you submitted your Problem Statement and EDA? If so, do you have the necessary data in hand (and the requisite EDA completed) to continue moving forward?\n",
    "\n",
    "Ans:\n",
    "\n",
    "\n",
    "    Yes, I am planning to change dataset from scraped Yelp data to already established Movielens dataset since the former has insufficient data for building a basic hybrid recommender. I have been spending time looking at online resources on how to incorporate modelling into a basic recommender, how to combine collaborative and content-based into a hybrid recommender as well as the type of evaluation metrics to be used and have not gotten much time for the EDA except for the following 2 plots. They both show that only very few movies are frequently rated, which leads to the need of only including movies with at least a considerable number of ratings and/or reviews (such as 100 for instance) into the building of the recommender. Now this was done before splitting the dataset into train and test sets. My question is, will doing this first-cut EDA on the full dataset lead eventually to high model variance since I am actually \"seeing\" part of the test dataset by conducting EDA on the entire dataset? \n",
    "    \n",
    "<img src=\"../../movielens_data/ratingdist_1.png\"/>\n",
    "<img src=\"../../movielens_data/ratingdist_2.png\"/>\n",
    "\n",
    "6. What is your timeline for the next week and a half? What do you _have_ to get done versus what would you _like_ to get done?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Have to get done:\n",
    "\n",
    "    1. (2-3 days): I hope to figure a way to do model-based collaborative filtering (OLS/ALS) to predict user ratings from train set and evaluate on test set using RMSE.\n",
    "    \n",
    "    \n",
    "    2. (2-3 days): I hope to figure a way to do content-based filtering (ensemble of OLS/ALS, where each OLS/ALS predicts only for 1 user) to predict user ratings from train set and evaluate on test set using RMSE.\n",
    "\n",
    "\n",
    "Have OR would like to get done:\n",
    "\n",
    "\n",
    "    3. (2-3 days): Depending on how well collaborative filtering and content-based filtering perform, attribute weights to each accordingly, weight each prediction and sum up to yield final model predictions for ratings and evaluate on test set using RMSE.\n",
    "\n",
    "\n",
    "Have OR would like to et done:\n",
    "\n",
    "\n",
    "    4. (1-2 days): Plug both user-item interaction matrix and movie feature vectors (eg. genres provided in movielens dataset) into a neural network (may rely on fastai library eventually if not enough time-downside is hard to extract all information I want such as RMSE from fastai library without much googling since it is a custom-build library developed by the FastAi lab in San Francisco.) and see how it compares to the manually combined hybrid recommender (points 1-3 above).\n",
    "\n",
    "\n",
    "7. What topics do you want to discuss during your 1:1?\n",
    "\n",
    "Ans:\n",
    "\n",
    "\n",
    "    1. Model-based Collaborative Filtering Modeling (OLS/ALS) walkthrough with a simple use case\n",
    "    \n",
    "    \n",
    "    2. Content-based Filtering Modeling (ensemble of OLS/ALS) walkthrough with a simple use case\n",
    "    \n",
    "    \n",
    "    3. How to design a neural network that can incorporate both user-item interaction matrix and item feature vector as inputs to predict user ratings in a neural network-based hybrid recommender (walkthrough with a simple use case would be appreciated) OR how to use fastai library for this purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
