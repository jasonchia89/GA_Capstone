{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "import random\n",
    "#import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from os import path   #uncomment these if you have downloaded and installed wordcloud based on the instructions above\n",
    "#from PIL import Image\n",
    "#from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "#from imblearn.over_sampling import ADASYN\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing X_legit and y which contain the shops that userid 2043 rated and ratings respectively\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 19307)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_legit = pd.read_csv('xlegit.csv')\n",
    "X_legit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4.0\n",
       "1    4.0\n",
       "2    5.0\n",
       "3    5.0\n",
       "4    4.0\n",
       "Name: user_rating, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv('y.csv', squeeze=True)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into train and test sets first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_legit, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    0.479592\n",
       "5.0    0.383929\n",
       "3.0    0.118622\n",
       "2.0    0.012755\n",
       "1.0    0.005102\n",
       "Name: user_rating, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate scaler since not all of the features are of the same scale, eg. review_count and avg_store_rating\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the train and transforming both the train and test sets\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "xgb_model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonchia/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8162284972411555"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just in case it takes too long to run this even...\n",
    "#cross_val_score(xgb_model,X_train_sc,y_train,cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    \n",
    "- ```cross_val_score``` was 0.82 already!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for initial decision trees tuned\n",
    "params = {\n",
    "    'learning_rate' : [0.01, 0.1, 0.3, 0.5, 0.9],\n",
    "    'n_estimators' : [20,50,100,200,500,1000],\n",
    "    'max_depth':[3,5,7,10],\n",
    "    'gamma':[0,1,2,3],\n",
    "    'max_delta_step':[1,2,3],\n",
    "    'scale_pos_weight':[0.5, 1, 2, 3]\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#took too long to run...to be run as part of future plans for this project\n",
    "#xgb_gridsearch = GridSearchCV(xgb_model, params, cv = 10, verbose = 1, n_jobs = -1)\n",
    "#xgb_gridsearch.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Gridsearch best score: ', xgb_gridsearch.best_score_)\n",
    "#print('Gridsearch best estimator: ', xgb_gridsearch.best_estimator_)\n",
    "#print('Gridsearch best score on test set: ', xgb_gridsearch.best_estimator_.score(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'xgb_model.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(xgb_gridsearch,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = joblib.load('xgb_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model.best_estimator_.score(X_test_sc,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating recommendations for userid 2043 based on XGB model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking X_test as first step in regenerating the shops column for predictions\n",
    "#trial = X_test.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating loop to re-generate original X_test order of shops\n",
    "#index_lst = []\n",
    "#outlets_lst = []\n",
    "#for n in range(len(trial.index)):\n",
    "#    if trial.index[n][1].startswith('shops_') and trial[n]!=0:\n",
    "#        index_lst.append(str(trial.index[n][0]))\n",
    "#        outlets_lst.append(trial.index[n][1])\n",
    "#index_lst = [int(x) for x in index_lst]\n",
    "#reconstructed_X_test = pd.DataFrame({'shops':outlets_lst}, index=index_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rating_predictions = loaded_model.best_estimator_.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructed_X_test['predicted_ratings']=rating_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructed_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructed_X_test['predicted_ratings'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructed_X_test['actual_ratings']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 5 recommendations for userid 2043 based on content-based filtering (using decision tree classifier) predictions\n",
    "#reconstructed_X_test.sort_values('predicted_ratings', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for evaluation of model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #defining function for obtaining tn, fp, fn, tp for each rating class for feeding into micro-avg precision and recall functions defined below\n",
    "# def cm_spec(y_true,y_pred,rating,state):\n",
    "#     if state=='tn':\n",
    "#         return multilabel_confusion_matrix(y_true,y_pred)[rating-1][0][0]\n",
    "#     elif state=='fp':\n",
    "#         return multilabel_confusion_matrix(y_true,y_pred)[rating-1][0][1]\n",
    "#     elif state=='fn':\n",
    "#         return multilabel_confusion_matrix(y_true,y_pred)[rating-1][1][0]\n",
    "#     else:\n",
    "#         return multilabel_confusion_matrix(y_true,y_pred)[rating-1][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #defining function for obtaining micro-avg precision\n",
    "# def micro_avg_precision(y_true,y_pred):\n",
    "#     return ((cm_spec(y_true,y_pred,1,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,2,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,3,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,4,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,5,'tp'))/(\n",
    "#                                                 cm_spec(y_true,y_pred,1,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,2,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,3,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,4,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,5,'tp')+\n",
    "#                                                 cm_spec(y_true,y_pred,1,'fp')+\n",
    "#                                                  cm_spec(y_true,y_pred,2,'fp')+\n",
    "#                                                  cm_spec(y_true,y_pred,3,'fp')+\n",
    "#                                                  cm_spec(y_true,y_pred,4,'fp')+\n",
    "#                                                  cm_spec(y_true,y_pred,5,'fp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #defining function for obtaining micro-avg recall\n",
    "# def micro_avg_recall(y_true,y_pred):\n",
    "#     return ((cm_spec(y_true,y_pred,1,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,2,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,3,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,4,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,5,'tp'))/(\n",
    "#                                                 cm_spec(y_true,y_pred,1,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,2,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,3,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,4,'tp')+\n",
    "#                                                  cm_spec(y_true,y_pred,5,'tp')+\n",
    "#                                                 cm_spec(y_true,y_pred,1,'fn')+\n",
    "#                                                  cm_spec(y_true,y_pred,2,'fn')+\n",
    "#                                                  cm_spec(y_true,y_pred,3,'fn')+\n",
    "#                                                  cm_spec(y_true,y_pred,4,'fn')+\n",
    "#                                                  cm_spec(y_true,y_pred,5,'fn')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #defining function for obtaining micro_avg_f1\n",
    "# def micro_avg_f1(y_true,y_pred):\n",
    "#     return 2 * ((micro_avg_precision(y_true,y_pred) * micro_avg_recall(y_true,y_pred))/(micro_avg_precision(y_true,y_pred) + micro_avg_recall(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def confusion_breakdown(y_true,y_pred,rating):\n",
    "#     print(\"True negatives for rating {}: {}\".format(\n",
    "#         rating,multilabel_confusion_matrix(y_true,y_pred)[rating-1][0][0]))\n",
    "#     print(\"False positives for rating {}: {}\".format(\n",
    "#         rating,multilabel_confusion_matrix(y_true,y_pred)[rating-1][0][1]))\n",
    "#     print(\"False negatives for rating {}: {}\".format(\n",
    "#         rating,multilabel_confusion_matrix(y_true,y_pred)[rating-1][1][0]))\n",
    "#     print(\"True positives for rating {}: {}\".format(\n",
    "#         rating,multilabel_confusion_matrix(y_true,y_pred)[rating-1][1][1]))\n",
    "#     return \"******************************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(confusion_breakdown(y_test,rating_predictions,1))\n",
    "#print(confusion_breakdown(y_test,rating_predictions,2))\n",
    "#print(confusion_breakdown(y_test,rating_predictions,3))\n",
    "#print(confusion_breakdown(y_test,rating_predictions,4))\n",
    "#print(confusion_breakdown(y_test,rating_predictions,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Gridsearched XGB yielded micro_avg_precision of \", micro_avg_precision(y_test,rating_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Gridsearched XGB yielded micro_avg_recall of \", micro_avg_recall(y_test,rating_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Gridsearched XGB yielded micro_avg_f1 of \", micro_avg_f1(y_test,rating_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification_report(y_test,rating_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the probabilities of predictions for above model\n",
    "#pred_proba_1 = [i[0] for i in loaded_model.best_estimator_.predict_proba(X_test_sc)]\n",
    "#pred_proba_2 = [i[1] for i in loaded_model.best_estimator_.predict_proba(X_test_sc)]\n",
    "#pred_proba_3 = [i[2] for i in loaded_model.best_estimator_.predict_proba(X_test_sc)]\n",
    "#pred_proba_4 = [i[3] for i in loaded_model.best_estimator_.predict_proba(X_test_sc)]\n",
    "#pred_proba_5 = [i[4] for i in loaded_model.best_estimator_.predict_proba(X_test_sc)]\n",
    "#pred_proba_all = pred_proba_1 + pred_proba_2 + pred_proba_3 + pred_proba_4 + pred_proba_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes with the true values and probabilities for each rating value\n",
    "#pred_df_1 = pd.DataFrame({'true_values': y_test,\n",
    " #                       'pred_probs':pred_proba_1})\n",
    "#pred_df_2 = pd.DataFrame({'true_values': y_test,\n",
    "#                        'pred_probs':pred_proba_2})\n",
    "#pred_df_3 = pd.DataFrame({'true_values': y_test,\n",
    "#                        'pred_probs':pred_proba_3})\n",
    "#pred_df_4 = pd.DataFrame({'true_values': y_test,\n",
    "#                        'pred_probs':pred_proba_4})\n",
    "#pred_df_5 = pd.DataFrame({'true_values': y_test,\n",
    "#                        'pred_probs':pred_proba_5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformating the pred_df for each of the 5 rating classes\n",
    "#pred_df_1['true_values'] = pred_df_1['true_values'].apply(lambda val:0 if val!=1 else 1)\n",
    "#pred_df_2['true_values'] = pred_df_2['true_values'].apply(lambda val:0 if val!=2 else 1)\n",
    "#pred_df_3['true_values'] = pred_df_3['true_values'].apply(lambda val:0 if val!=3 else 1)\n",
    "#pred_df_4['true_values'] = pred_df_4['true_values'].apply(lambda val:0 if val!=4 else 1)\n",
    "#pred_df_5['true_values'] = pred_df_5['true_values'].apply(lambda val:0 if val!=5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a pred_df for all classes as a whole for computing the micro-avg ROC AUC later on...\n",
    "#pred_df_all = pd.DataFrame({'true_values': pred_df_1['true_values'].tolist()+pred_df_2['true_values'].tolist()+\n",
    " #                           pred_df_3['true_values'].tolist()+pred_df_4['true_values'].tolist()+\n",
    " #                           pred_df_5['true_values'].tolist(),\n",
    " #                       'pred_probs':pred_proba_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate roc score for the respective rating values\n",
    "#roc_score_1 = roc_auc_score(pred_df_1['true_values'], pred_df_1['pred_probs'], multi_class='ovo')\n",
    "#print(\"The ROC AUC for rating 1 is \\n{}\".format(roc_score_1))\n",
    "#roc_score_2 = roc_auc_score(pred_df_2['true_values'], pred_df_2['pred_probs'], multi_class='ovo')\n",
    "#print(\"The ROC AUC for rating 2 is \\n{}\".format(roc_score_2))\n",
    "#roc_score_3 = roc_auc_score(pred_df_3['true_values'], pred_df_3['pred_probs'], multi_class='ovo')\n",
    "#print(\"The ROC AUC for rating 3 is \\n{}\".format(roc_score_3))\n",
    "#roc_score_4 = roc_auc_score(pred_df_4['true_values'], pred_df_4['pred_probs'], multi_class='ovo')\n",
    "#print(\"The ROC AUC for rating 4 is \\n{}\".format(roc_score_4))\n",
    "#roc_score_5 = roc_auc_score(pred_df_5['true_values'], pred_df_5['pred_probs'], multi_class='ovo')\n",
    "#print(\"The ROC AUC for rating 5 is \\n{}\".format(roc_score_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc_score_all = roc_auc_score(pred_df_all['true_values'], pred_df_all['pred_probs'], multi_class='ovo')\n",
    "#print(\"The ROC AUC for all ratings in general is \\n{}\".format(roc_score_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to calculate sensitivity. (True positive rate.)\n",
    "# def TPR(df, true_col, pred_prob_col, threshold):\n",
    "#     true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "#     false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "#     return true_positive / (true_positive + false_negative)\n",
    "\n",
    "# # Define function to calculate 1 - specificity. (False positive rate.)\n",
    "# def FPR(df, true_col, pred_prob_col, threshold):\n",
    "#     true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "#     false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "#     return 1 - (true_negative / (true_negative + false_positive))\n",
    "\n",
    "# # Define function to calculate micro-averaged sensitivity. (Micro-averaged True positive rate.)\n",
    "# def micro_avg_TPR(df1,df2,df3,df4,df5, true_col, pred_prob_col, threshold):\n",
    "#     true_positive1 = df1[(df1[true_col] == 1) & (df1[pred_prob_col] >= threshold)].shape[0]\n",
    "#     true_positive2 = df2[(df2[true_col] == 1) & (df2[pred_prob_col] >= threshold)].shape[0]\n",
    "#     true_positive3 = df3[(df3[true_col] == 1) & (df3[pred_prob_col] >= threshold)].shape[0]\n",
    "#     true_positive4 = df4[(df4[true_col] == 1) & (df4[pred_prob_col] >= threshold)].shape[0]\n",
    "#     true_positive5 = df5[(df5[true_col] == 1) & (df5[pred_prob_col] >= threshold)].shape[0]\n",
    "#     false_negative1 = df1[(df1[true_col] == 1) & (df1[pred_prob_col] < threshold)].shape[0]\n",
    "#     false_negative2 = df2[(df2[true_col] == 1) & (df2[pred_prob_col] < threshold)].shape[0]\n",
    "#     false_negative3 = df3[(df3[true_col] == 1) & (df3[pred_prob_col] < threshold)].shape[0]\n",
    "#     false_negative4 = df4[(df4[true_col] == 1) & (df4[pred_prob_col] < threshold)].shape[0]\n",
    "#     false_negative5 = df5[(df5[true_col] == 1) & (df5[pred_prob_col] < threshold)].shape[0]\n",
    "#     return (true_positive1+true_positive2+true_positive3+true_positive4+true_positive5) / (true_positive1+true_positive2+true_positive3+true_positive4+true_positive5 + \n",
    "#  false_negative1+false_negative2+false_negative3+false_negative4+false_negative5)\n",
    "\n",
    "# # Define function to calculate micro-averaged false positive rate. (Micro-averaged (1-specificity))\n",
    "# def micro_avg_FPR(df1,df2,df3,df4,df5, true_col, pred_prob_col, threshold):\n",
    "#     true_negative1 = df1[(df1[true_col] == 0) & (df1[pred_prob_col] <= threshold)].shape[0]\n",
    "#     true_negative2 = df2[(df2[true_col] == 0) & (df2[pred_prob_col] <= threshold)].shape[0]\n",
    "#     true_negative3 = df3[(df3[true_col] == 0) & (df3[pred_prob_col] <= threshold)].shape[0]\n",
    "#     true_negative4 = df4[(df4[true_col] == 0) & (df4[pred_prob_col] <= threshold)].shape[0]\n",
    "#     true_negative5 = df5[(df5[true_col] == 0) & (df5[pred_prob_col] <= threshold)].shape[0]\n",
    "#     false_positive1 = df1[(df1[true_col] == 0) & (df1[pred_prob_col] > threshold)].shape[0]\n",
    "#     false_positive2 = df2[(df2[true_col] == 0) & (df2[pred_prob_col] > threshold)].shape[0]\n",
    "#     false_positive3 = df3[(df3[true_col] == 0) & (df3[pred_prob_col] > threshold)].shape[0]\n",
    "#     false_positive4 = df4[(df4[true_col] == 0) & (df4[pred_prob_col] > threshold)].shape[0]\n",
    "#     false_positive5 = df5[(df5[true_col] == 0) & (df5[pred_prob_col] > threshold)].shape[0]\n",
    "#     return 1 - ((true_negative1+true_negative2+true_negative3+true_negative4+true_negative5) / \n",
    "# (true_negative1+true_negative2+true_negative3+true_negative4+true_negative5 + \n",
    "#  false_positive1+false_positive2+false_positive3+false_positive4+false_positive5))\n",
    "\n",
    "\n",
    "\n",
    "# # Function to plot the ROC curve\n",
    "# def roc_plot(fpr_values_1,tpr_values_1,\n",
    "#              fpr_values_2,tpr_values_2,\n",
    "#              fpr_values_3,tpr_values_3,\n",
    "#              fpr_values_4,tpr_values_4,\n",
    "#              fpr_values_5,tpr_values_5,\n",
    "#              fpr_values_all,tpr_values_all):\n",
    "#     # Create figure.\n",
    "#     plt.figure(figsize = (10,7))\n",
    "\n",
    "#     # Create threshold values. (Dashed red line in image.)\n",
    "#     thresholds = np.linspace(0, 1, 200)    \n",
    "\n",
    "#     # Plot ROC curve for rating 1.\n",
    "#     plt.plot(fpr_values_1, # False Positive Rate on X-axis\n",
    "#              tpr_values_1, # True Positive Rate on Y-axis\n",
    "#              label='Rating 1 ROC Curve with AUC = {}'.format(round(roc_score_1,3)))\n",
    "    \n",
    "#     # Plot ROC curve for rating 2.\n",
    "#     plt.plot(fpr_values_2, # False Positive Rate on X-axis\n",
    "#              tpr_values_2, # True Positive Rate on Y-axis\n",
    "#              label='Rating 2 ROC Curve with AUC = {}'.format(round(roc_score_2,3)))\n",
    "    \n",
    "#     # Plot ROC curve for rating 3.\n",
    "#     plt.plot(fpr_values_3, # False Positive Rate on X-axis\n",
    "#              tpr_values_3, # True Positive Rate on Y-axis\n",
    "#              label='Rating 3 ROC Curve with AUC = {}'.format(round(roc_score_3,3)))\n",
    "    \n",
    "#     # Plot ROC curve for rating 4.\n",
    "#     plt.plot(fpr_values_4, # False Positive Rate on X-axis\n",
    "#              tpr_values_4, # True Positive Rate on Y-axis\n",
    "#              label='Rating 4 ROC Curve with AUC = {}'.format(round(roc_score_4,3)))\n",
    "    \n",
    "#     # Plot ROC curve for rating 5.\n",
    "#     plt.plot(fpr_values_5, # False Positive Rate on X-axis\n",
    "#              tpr_values_5, # True Positive Rate on Y-axis\n",
    "#              label='Rating 5 ROC Curve with AUC = {}'.format(round(roc_score_5,3)))\n",
    "    \n",
    "#     # Plot ROC curve for all ratings in general (micro-averaged).\n",
    "#     plt.plot(fpr_values_all, # False Positive Rate on X-axis\n",
    "#              tpr_values_all, # True Positive Rate on Y-axis\n",
    "#              label='Micro-Averaged ROC Curve with AUC = {}'.format(round(roc_score_all,3)))\n",
    "    \n",
    "    \n",
    "#     # Plot baseline. (Perfect overlap between the two populations.)\n",
    "#     plt.plot(np.linspace(0, 1, 200),\n",
    "#              np.linspace(0, 1, 200),\n",
    "#              label='baseline',\n",
    "#              linestyle='--')\n",
    "\n",
    "#     # Label axes.\n",
    "#     #plt.title(f'ROC Curve with AUC = {round(roc_auc_score(pred_df['true_values'],pred_df['pred_probs']),3)}', fontsize=22)\n",
    "#     plt.title('Multi-class ROC AUC', fontsize=15)\n",
    "#     plt.ylabel('Sensitivity', fontsize=12)\n",
    "#     plt.xlabel('1 - Specificity', fontsize=12)\n",
    "\n",
    "#     # Create legend.\n",
    "#     plt.legend(fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "\n",
    "# # Create threshold values. (Dashed red line in image.)\n",
    "# thresholds = np.linspace(0, 1, 200)  \n",
    "\n",
    "# # Call the function to create true_positive_rate values\n",
    "# tpr_values_1 = [TPR(pred_df_1, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# tpr_values_2 = [TPR(pred_df_2, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# tpr_values_3 = [TPR(pred_df_3, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# tpr_values_4 = [TPR(pred_df_4, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# tpr_values_5 = [TPR(pred_df_5, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# tpr_values_all = [micro_avg_TPR(pred_df_1,pred_df_2,pred_df_3,pred_df_4,pred_df_5, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# # Call the function to create false_positive_rate values\n",
    "# fpr_values_1 = [FPR(pred_df_1, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# fpr_values_2 = [FPR(pred_df_2, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# fpr_values_3 = [FPR(pred_df_3, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# fpr_values_4 = [FPR(pred_df_4, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# fpr_values_5 = [FPR(pred_df_5, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "# fpr_values_all = [micro_avg_FPR(pred_df_1,pred_df_2,pred_df_3,pred_df_4,pred_df_5, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "\n",
    "# # Plot the graph\n",
    "# roc_plot(fpr_values_1,tpr_values_1,\n",
    "#          fpr_values_2,tpr_values_2,\n",
    "#          fpr_values_3,tpr_values_3,\n",
    "#          fpr_values_4,tpr_values_4,\n",
    "#          fpr_values_5,tpr_values_5,\n",
    "#          fpr_values_all,tpr_values_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    \n",
    "- Tuned XGBClassifier______"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
